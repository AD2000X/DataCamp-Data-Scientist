# Understanding model optimization
## Why optimization is hard
- simultaneously optimizing 1000s of parameters with complex relationships
- updates may not imporve model meaningfully
- updates too small(if learning rate is low) or too large(if learning rate is high)

__stochastic gradient descent__
> def get_new_model(input_shape = input_shape):  
> model = Sequential()  
> model.add(Dense(100, activation = 'relu', input_shape = input_shape))  
> model.add(Dense(100, activation = 'relu))  
> model.add(Dense(2, activation = 'softmax'))  
> return(model)

> lr_to_test = [0.0000001, 0.01, 1]  
> #loop over learning rates  
> for lr in lr_to_test:  
> model = get_new_model()  
> my_optimizer = SGD(lr = lr)  
> model.compile(optimizer = my_optimizer, loss = 'categorical_corssentropy')  
> model.fit(predictors, target)

## The dying neuron problem
- once a node starts always getting negative inputs
  - it may continue only getting negative inputs
- contributes nothing to the model
  - 'Dead' neuron

## Vanishing gradients
- occurs when many layers have very small slopes(e.g. due to being on flat part of tanh curve)
- in deep networks, updates to backprop were close to 0

# Model validation
## Validation in deep learning
- commonly use validation split split rather than cross-validation
- deep learning widely used on large datasets
- single validation score is based on large amount of data, and is reliable
- repeated training from cross-validation would take long time

__model validation__
> model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])  
> model.fit(predictors, target, validation_split= 0.3)

__Early stopping__
> from keras.callbacks import EarlyStopping  
> early_stopping_monitor = EarlyStopping(patience = 2)  
> model.fit(predictors, target, validation_split = 0.3, epochs = 20, callbacks = [early_stopping_monitor])

__Experimentation__
- experiment with different architectures
- more layers
- fewer layers
- layers with more nodes
- layers with fewer nodes
- creating a great model requires experimentation

# Thinking about model capacity
## Workflow for optimizing model capacity
- start with a small network
- gradually increase capacity
- keep increasing capacity until validation score is no longer improving

# Stepping up to images
## Recognizing handwritten digits
- MNIST dataset
- 28*28 grid flattened to 784 values for each image
- value in each part of array denotes darkness of that pixel

# Final thoughts
## Next steps
- start with standard prediction problems on tables of numbers
- images(with convolutional neural networks) are common next steps
- keras.io for excellent documentation
- graphical processing unit(GPU) provides dramatic speedups in model training times
- need a CUDA compatible GPU
- for training on using GPUs in the clousd look here: http://bit.ly/2mYQXQb