# AdaBoost
## Boosting
- __Boosting__: ensemble method combining several weak learners to form a strong learner
- __weak learner__: model doing slightly better than random guessing
- example of weak learner: Decision stump(CART whose maximum depth is 1)

<br>

- train an ensemble of predictors sequentially
- each predictor tires to correct its predecessor
- most popular boosting methods:
  - AdaBoost
  - Gradient Boosting

## AdaBoost
- Stand for **Ada**ptive  **Boost**ing
- each predictor pays more attention to the instances wrongly predicted by its predecessor
- achieved by changing the weight of training instances
- each predictor is assigned a coefficient $a$
- $a$ depends on the predictor's training error

## AdaBoost: Prediction
- Classification:
  - weighted majority voting
  - in sklearn: AdaBoostClassifier

- Regression:
  - weighted average
  - in sklearn: AdaBoostRegressor

__AdaBoost Classification in sklearn__
> from sklearn.ensemble import AdaBoostClassifier  
> from sklearn.tree import DecisionTreeClassifier  
> from sklearn.metrics import roc_auc_score  
> from sklearn.model_selection import train_test_split  
> seed = 1  
> X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, stratify = y, random_state = seed)  

> #instantiate a classification-tree 'dt'  
> dt = DecisionTreeClassifier(max_depth = 1, random_state = seed)  
> #instantiate an AdaBoost classifier 'adb_clf'  
> adb_clf = AdaBoostClassifier(base_estimator = dt, n_estimators = 100)  
> adb_clf.fit(X_train, y_train)  
> #predict the test set probabilities of positive class
> y_pred_proba = adb_clf.predict_proba(X_test)[:, 1]  
> #evaluate test-set roc_auc_score  
> adb_clf_roc_auc_score = roc_auc_score(y_test, y_pred_proba)  
> print('ROC AUC score: {:.2f}'.format(adb_clf_roc_auc_score))  
> [out] ROC AUC score: 0.99

# Gradient Boosting(GB)
## Gradient Boosted Trees
- sequential correction of predecessor's errors
- does not tweak the weights of training instances
- fit each predictor is trained using its predecessor's residual errors as labels
- Gradient Boosted Trees: a CART is used as a base learner

## Gradient Boosted Trees: Prediction
- Regression:
  - $y_{pred} = y_1 + nr_1 + ... + nr_N$
  - in sklearn: GradientBoostingRegressor
  
- Classification:
  - in sklearn: GradientBoostingClassifier

__Gradient Boosting in sklearn(auto dataset)__
> #import models and utility functions  
> from sklearn.ensemble import GradientBoostingRegressor  
> from sklearn.model_selection import train_test_split  
> from sklearn.metrics import mean_squared_error as MSE  
> seed = 1  
> X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = seed)  

> #instantiate a GradientBoostingRegressor 'gbt'  
> gbt = GradientBoostingRegressor(n_estimator = 300, max_depth = 1, random_state = seed)  
> gbt.fit(X_train, y_train)  
> y_pred = gbt.predict(X_test)  
> rmse_test = MSE(y_test, y_pred)**(0.5)  
> print('test set RMSE: (:.2f)'.format(rmse_test))  
> [out] test set RMSE: 4.01

# Stochastic Gradient Boosting(SGB)
## Gradient Boosting: Cons
- GB involves an exaustive search procedure
- each CART is trained to find the best split points and features
- may lead to CARTs using the same split points and maybe the same features

## Stochastic Gradient Boosting
- each tree is trained on a random subsets of rows of the training data
- the sampled instances(40%~80% of the training set) are sampled without replacement
- features are sampled(without replacement) when choosing split points
- result: further ensemble diversity
- effect: adding further variance to the ensemble of trees

__Stochastic Gradient Boosting in sklearn(auto dataset)__
> #import models and utility functions  
> from sklearn.ensemble import GradientBoostingRegressor  
> from sklearn.model_selection import train_test_split  
> from sklearn.metrics import mean_squared_arror as MSE  
> seed = 1  
> X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = seed)

> #instantiate a stochastic GradientBoostingRegressor 'sgbt'  
> sgbt = GradientBoostingRegressor(max_depth = 1, subsmaple = 0.8, max_features = 0.2, n_estimators = 300, random_state = seed)  
> sgbt.fit(X_train, y_train)  
> y_pred = sgbt.predict(X_test)  

> #evaluate test set RMSE 'rmse_test'  
> rmse_test = MSE(y_test, y_pred)**(0.5)  
> print('test set RMSE: {:.2f}'.format(rmse_test))  
> [out] test set RMSE: 3.95