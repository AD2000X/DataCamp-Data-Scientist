# Non-negative matrix factorization(NMF)
## Non-negative matrix factorization
- NMF = 'non-negative matrix factorization'
- dimension reduction technique
- NMF models are interpretable(unlike PCA)
- easy to interpret means easy to explain!
- however, all sample features must be non-negative(>=0)

## Interpretable parts
- NMF expresses documents as combinations of topics(or 'themes')
- NMF express images as combinations of patterns

## Using scikit-learn NMF
- follow fit()/transform() pattern
- must specify numbers of components e.g. NMF(n_components = 2)
- work with NumPy arrays and with csr_matrix

## Example word-frequency array
- word frequency array, 4 words, many documents
- measure presence of words in each documents using 'tf-idf'
- 'tf' = frequency of word in document
- 'idf' reduces influence of frequent words

## Example usage of NMF
- samples is the word-frequency array
> from sklearn.decomposition import NMF  
> model = NMF(n_components = 2)  
> model.fit(samples)  
> [out] NMF(alpha = 0.0, ...)  
> nmf_features = model.transform(samples)

## NMF components
- NMF has components
- ... just like PCA has principal components
- dimension of components = dimension of samples
- entries are non-negative
- use print(model.components_) to check

## NMF features
- NMF feature values are non-negative
- can be used to reconstruct the samples
- ... combine feature values with components
- use print(nmf_features) to check

## Sample reconstruction
- multiply components by feature values, and add up
- can also be express as a product of matrices
- this is the 'Matrix Factorization' in 'NMF'

## NMF fits to non-negative data, only
- word frequencies in each document
- images encoded as arrays
- audio spectrograms
- purchase histories on e-commerce sites
- ... and many more!

# NMF learns interpretable parts
## Example: NMF learns interpretable parts
- word-frequency array articles(tf-idf)
- 20,000 scientific articles(rows)
- 800 words(columns)

## Applying NMF to the articles
> print(articles.shape)  
> [out] (20000, 800)  
> from sklearn.decomposition import NMF  
> nmf = NMF(n_components = 10)  
> nmf.fit(articles)  
> [NMF(alpha = 0.0, ...)]  
> print(nmf.components_.shape)  
> [out] (10, 800)

## NMF components
- for documents:
  - NMF components represent topics
  - NMF features combine topics into documents
- for images, NMF components are part of images

## Grayscale images
- 'grayscale' image = no colors, only shades of gray
- measure pixel brightness
- represent with value between 0 and 1 (0 is black)
- convert to 2D array

## Grayscale images as flat arrays
- enumerate the entries
- row-by-row
- from left to right

## Encoding a collection of images
- collection of images of the same size
- encode as 2D array
- each row corresponds to an image
- each column corresponds to a pixel
- ... can apply NMF!

__Visualizing samples__
>  print(samples)  
> [0. 1. 0.5 1. 0. 1.]  
> bitmap = sample.reshape((2,3))  
> print(bitmap)  
> [out] [[0. 1. 0.5]  
> [1. 0. 1. ]]  
> from matplotlib import pyplot as plt  
> plt.imshow(bitmap, cmp = 'gray', interpolation = 'nearest')  
> plt.show()

# Building recommender systems using NMF
## Finding similar articles
- enginner at a large online newspaper
- task: recommend articles similar to article being read by customer
- similar articles should have similar topics

## Strategy
- Apply NMF to the word-frequency array
- NMF feature values describe the topics
- ... so similar documents have similar NMF feature values
- compare NMF feature values?

__apply NMF to the word-frequency array__
- articles is a word frequency array
> from sklearn.decomposition import NMF  
> nmf = NMF(n_components = 6)  
> nmf_features = nmf.fit_transform(articles)

- compare NMF feature values?

## versions of articles
- different version of the same document have same topic proportions
- ... exact feature values may be different!
- e.g. because one version uses many meaningless words
- but all versions lie on the same line through the origin

## Cosine similarity
- use the angle between the lines
- higher values means more similar
- maximum value is 1, when angel is 0

## Calculating the cosine similarities
> from sklearn.preprocessing import normalize  
> norm_features = normalize(nmf_features)  
> current_article = norm_features[23,:] #if has index 23  
> similarities = norm_features.dot(current_article)  
> print(similarities)  
> [out] [ 0.71 0.26 0.42 ... 0.20 0.04]

## DataFrames and labels
- label similarities with the article titles, using a DataFrame
- titles given as a list: titles
> import pandas as pd  
> norm_features = normalize(nmf_features)  
> df = pd.DataFrame(norm_features, index = titles)  
> current_article = df.loc['Dog bites man']  
> similarities = df.dot(current_article)

> print(similarities.nlargest())  
> [out]  
> dog bites man 1.0000  
> hound mauls cat 0.9799  
> pets go wild! 0.9797  
> ...