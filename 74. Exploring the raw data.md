# Introducing the challenge
- learn from the expert who won DrivenData's challenge
  - natural language processing
  - feature engineering
  - efficiency boosting hashing tricks
- use data to have a social impact
- Budgets for schools are huge, complex, and not standardized
  - hundreds of hours each year are spend manually labelling
- Goal: build a machine learning algorithm that can automate the process
- budget data
  - line-item: "Algebra books for 8th grade students"
  - labels: "Textbooks", "math", "middle school"
- this is a supervised learning problem
- over 100 target variables!
  - this is a classification problem
- how can we help
  - predictions will be probabilities for each label

# Exploring the data
__load and preview the data__
> import pandas as pd  
> sample_df = pd.read_csv('sample_data.csv')  
> sample.df.head()  

__summarize the data__
> sample_df.info()  
> sample_df.describe()

# Looking at the datatypes
__objects instead of categories__
> sample_df['label'].head()
> [out] aaaba

## Encode labels as categories
- ML algorithms work on numbers, not string
  - need a numeric representation of these string
- strings can be slow compared to numbers
- in pandas, 'categroy' dtype encodes categorical data numerically
  - can speed up code

__Encode labels as categories(sample data)__
> sample_df.label.head(2)  
> ab  
> sample_df.label = sample)df.label.astype('category')  
> ab

__dummy variable encoding__
> dummies = pd.get_dummies(sample_df[['label']], prefix_sep = '_')  
> dummies.head(2)  

[out]  
label_a | label_b
--------|----------
1       |0
0       |1

__lambda functions__
- alternative to 'def' syntax
- easy way to make simple, one-line functions
> square = lambda x: x*x  
> square(2)  
> [out] 4

## encode label as categories
- in the sample dataframe, we only have one relevant column
- in the budget data, there are multiple columns that need to be made categorical

__encode labels as categories__
> categorize_label = lambda x: x.astype('category')  
> sample_df.label = sample_df[['label']].apply(categorize_label, axis = 0)

NOTE:  
- df.dtypes.value_counts()
- pd.Series.nunique

# How do we measure success?
- accuracy can be misleading when classes are imbalanced
  - legitimate email: 99%, spam:1%
  - model that never predict spam will be 99% accurate!
- metric used in this problem: log loss
  - it is a loss function
  - measure of error
  - want to minimize the error(unlike accuracy)

## Log loss binary classification
- log loss for binary classification
  - actual value: y = {1 = yes, 0 = no}
  - prediction (probability that the value is 1): p
  
## Log loss binary classification: example


$$logloss = -\frac{1}{N}\sum_{i = 1}^{N}(y_{i}log(p_{i})+(1-y_{i})log(1-p_{i}))$$

- true label = 0
- model confidently predict 1 (with p = 0.90)
- log loss = (1-y)log(1-p) = log(1-0.9) = log(0.1) = 2.3
<br>
<br>
<br>
- true label = 1
- model predicts 0(with p = 0.5) 
- log loss = 0.69
- better to be less confident then confident and wrong

__computing log loss with NumPy__
> import numpy as np  
> def compute_log_loss(predicted, actual, eps = 1e-14):  
> "compute the logarithmic loss between predicted and actual when these are 1D array"  
> predicted = np.clip(predicted, eps, 1-eps)  
> loss = -1 * np.mean(actual * np.log(predicted) * (1 - actual) * np.log(1 - predicted))  
> return loss

> compute_log_loss(predicted = 0.9, actual = 0)  
> [out] 2.302  
> compute_log_loss(predicted = 0.5, actual = 1)  
> [out] 0.693