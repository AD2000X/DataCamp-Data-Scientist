# Introduction to regression
__creating feature and target arrays__
> boston = pd.read_csv('boston.csv')
> x = boston.drop('MEDV', axis = 1).values  
> y = boston['MEDV'].values

__predicting house value from a single feature__
> x_rooms = x[:,5]  
> type(x_rooms), type(y)  
> [out] numpy.ndarray, numpy.ndarray  
> y = y.reshape(-1,1)  
> x_rooms = x_rooms.reshape(-1,1)

__plotting house values vs. number of rooms__
> plt.scatter(x_rooms, y)  
> plt.ylabel('value of house')  
> plt.xlabel('number of rooms')  
> plt.show()  

__fitting a regression model__
> import numpy as np  
> from sklearn import linear_model  
> reg = linear_model.LinearRegression()  
> reg.fit(x_rooms, y)  
> prediction_space = np.linspace(min(x_rooms), max(x_rooms).reshape(-1,1))  
> plt.scatter(x_rooms, y, color = 'blue')  
> plt.plot(prediction_space, reg.predict(prediction_space), color = 'black', linewidth = 3)  
> plt.show()

# The basic of linear regression
## Regression mechanics
- y = ax + b  
  - y = target
  - x = single feature
  - a,b = parameters of model
- how do we choose a and b?
- define an error function for any given line
  - choose the line that minizes the error function

## The loss function
- ordinary least square (OLS): Minimize sum of square of residuals.

## Linear regression in higher dimensions
### y = a1x1 + a2x2 + b
- to fit a linear regression model here:
  - need to specify 3 variables
- in higher dimensions:
  - y = a1x1 + a2x2 + a3x3 + anxn + b
  - must specify coefficient for each feature and the variable b
- Scikit-learn API work exactly the same way:
  - pass two arrays: Features and Target

__Linear regression on all features__
> from skearn.model_selection import train_test_split  
> X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 42)  
> reg_all = linear_model.LinearRegression()  
> reg_all.fit(X_train, y_train)  
> y_pred = reg_all.predict(X_test)  
> reg_all.score(X_test, y_test)  
> [out]  0.711

# Cross-validation
## Cross-validation motivation
- model performance is dependent on way the data is split
- not representative of the model's ability to generalize 
- solution: cross-validation
- split data into k group and match each seperatly. k folds = k-fold CV
- more folds = more computationally expensive
__cross-validation in scikit-learn__
> from sklearn.model_selection import cross_val_score  
> reg = linear_model.LinearRegression()  
> cv_result = cross_val_score(reg, X, y, cv = 5)  
> print(cv_results)  
> [out] [0.63 0.71 0.58 0.079 -0.25]  
> np.mean(cv_results)  
> [out] 0.35

# Regularized regression
## Why regularize?
- recall: linear regression minimizes a loss function
- it choose a coefficient for each feature variable
- large coefficients can lead to overfitting
- penalizing large coefficients: regularization

## Ridge regression
- loss function = OLS loss function + $a * \sum_{i = 1}^{n}a_{i}^{2}$
- alpha: parameter we need to choose
- picking alpha here is similar to picking k in k-NN
- hyperparameter tuning(more in chapter 3)  
- alpha controls model complexity
  - alpha = 0: we get back OLS(can lead to overfitting)
  - very high alpha: can lead to underfitting

__ridge regression in scikit-learn__
> from sklearn.linear_model import Ridge  
> X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)  
> ridge = Ridge(alpha = 0.1, normalize = True)  
> ridge.fit(X_train, y_train)  
> ridge_pred = ridge.predict(X_test)  
> ridge.score(X_test, y_test)  
> [out] 0.699  

## Lasso regression
- loss function = OLS loss function + $a * \sum_{i = 1}^{n}|a_{i}|$

__lasso regression in scikit-learn__
> from sklearn.linear_model import Lasso  
> X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)  
> lasso = Lasso(alpha = 0.1, normalize = True)  
> lasso.fit(X_train, y_train)  
> lasso_pred = lasso.predict(X_test)  
> lasso.score(X_test, y_test)  
> [out] 0.598

## Lasso regression for features selection
- can be used to select important features of a dataset
- shrinks the coefficients of less important features to exacctly 0

__lasso for feature selection in scikit-learn__
> from skearn.linear_model import Lasso  
> names = boston.drop('MEDV', axis = 1).columns  
> lasso = Lasso(alpha = 0.1)  
> lasso_coef = lasso.fit(X, y).coef_  
> _ = plt.plot(range(len(names)), lasso_coef)  
> _ = plt.xticks(range(len(names)), names, rotation = 60)  
> _ = plt.ylabel('coefficients')  
> plt.show()