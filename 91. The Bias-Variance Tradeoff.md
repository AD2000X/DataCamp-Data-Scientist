# Generalization Error
## Supervised Learning - Under the Hood
- Supervised Learning: $y = f(x)$, $f$ is unknown

## Goals of Supervised Learning
- find a model $\hat{f}$ that best approximates $f$: $\hat{f}$ $\approx$ $f$
- $\hat{f}$ can be Logistic Regression, Decision Tree, Neural Network ...
- Discard noise as much as possible
- End goal: $\hat{f}$ should acheive a low predictive error on unseen datasets.

## Difficulties in approximating $f$
- overfitting
  - $\hat{f}$(x) fits the training set noise
- underfitting
  - $\hat{f}$ is not flexible enough to approximate $f$

## Generalization Error
- Generalization Error of $\hat{f}$:  does $\hat{f}$ generalize well on unseen data?
- it can be decomposed as follows:
  - Generalization Error of $\hat{f}$ = $bias^2 + variance +$ irreducible error

## Terms
- Bias: error term that tells you, on average, how much $\hat{f} \neq f$ 
- Variance: tells you how much $\hat{f}$ is inconsistent over different training sets.
- Model Complexity
  - Model complexity: sets the flexibility of $\hat{f}$
  - ex: maximum tree depth, minimum samples per leaf, ...
- Bias-Variance Tradeoff

# Diagnosing Bias and Variance Problems
## Estimating the Generalization Error
- how do we estimate the generalization error of a model?
- cannot be done directly because
  - $f$ is unknown
  - usually you only have one dataset
  - noise is unpredictable
- Solution
  - split the data to training and test set
  - fit $f$ to the training set
  - evaluate the error of $\hat{f}$ on the __unseen__ test set
  - generalization error of $\hat{f} \approx$  test set error of $\hat{f}$

## Better model evaluation with Cross-Validation
- test set should not be touched until we are confident about $\hat{f}$'s performance
- evaluating $\hat{f}$ on training set: biased estimate, $\hat{f}$ has already seen all training points.
- Solution $->$ Cross-Validation(CV):
  - K-Fold CV
  - Hold-Out CV

## Diagnose Variance Problems(too much features)
- if $\hat{f}$ suffers from high variance:
  - CV error of $\hat{f}$ > training set error of $\hat{f}$
- $\hat{f}$ is said to overfit the training set. To remedy overfitting:
  - decrease model complexity
  - for ex: decrease max depth, increase min samples per leaf, ...
  - gather more data...

## Diagnose Bias Problems(too little features)
- if $\hat{f}$ suffer from high bias:
  - CV error of $\hat{f} \approx$  training set error of $\hat{f}$ >> desired error
  - $\hat{f}$ said to underfit the training set. To remedy underfitting:
    - increase model complexity
    - for ex: increase max depth, decrease min samples per leaf, ...
    - gather more relevant features

__K-Fold CV in sklearn on the Auto Dataset__
> from sklearn.tree import DecisionTreeRegressor  
> from sklearn.model_selection import train_test_split  
> from sklearn.metrics import mean_squared_error as MSE  
> from sklearn.model_selection import cross_val_score  
> seed = 123  
> X_train,X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_stats = seed)  
> dt = DecisionTreeRegressor(max_depth = 4, min_samples_leaf = 0.14, random_state = seed)

> #evaluate the list of MSE ontained by 10-fold CV  
> #set n_jobs to -1 in order to exploit all CPU cores in computation  
> MSE_CV = - cross_val_score(dt, X_train, y_train, cv = 10, scoring = 'neg_mean_squared_error', n_jobs = -1)  
> dt.fit(X_train, y_train)  
> y_predict_train = dt.predict(X_train)
> y_predict_test = dt.predict(X_test)

# Ensemble Learning
## Advantages of CARTs
- simple to understand
- simple to interpret
- easy to use
- Flexibility: ability to dexcribe non-linear dependencies
- preprocessing: no need to standardize or normalize features,...

## Limitations of CARTs
- Classification: can only produce orthogonal decision boundaries
- sensitive to small variations in the training set
- high variance: unconstrained CARTs may overfit the training set
- solution: ensemble learning

## Ensemble Learning
- train different models on the same dataset
- let each model make its predictions
- meta-moddel: aggregates predictions of individual models
- final prediction: more robust and less prone to errors
- best results: models are skillful in different ways

## Ensemble learning in Practice: Voting Classifier
- binary classification task
- N classifiers make predictions: P1, P2, ... Pm with Pi = 0 or 1.
- meta-model prediction: hard voting

## Voting Classifier in sklearn(Breast-Cancer dataset)
> from sklearn.metrics import accuracy_score  
> from sklearn.model_selection import train_test_split  
> from sklearn.linear_model import LogisticRegression  
> from sklearn.tree import DecisionTreeClassifier  
> from sklearn.neighbors import KNeighborsClassifier as KNN  
> from sklearn.ensemble import VotingClassifier  
> seed = 1

> X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = seed)  
> #instantiate individual classifiers  
> lr = LogisticRegression(random_state = seed)  
> knn = KNN()  
> dt = DecisionTreeClassifier(random_state = seed)  
> #define a list called classifier that contains the tuples(classifier_name, classifier)  
> classifiers = [('Logistic Regression', lr), ('K Nearest Neighbors', knn), ('Classification Tree', dt)]  
> #iterate over the defined list of tuples containing the classifiers  
> for clf_name, clf in classifiers:  
> #fit clf to the training set  
> clf.fit(X_train,y_train)  
> #predict the labels of the test set  
> y_pred = clf.predict(X_test)  
> #Evaluate the accuracy of clf on the test set  
> print('{:s} : {:.3f}'.format(clf_name, accuracy_score(y_test, y_pred)))  
> [out]  
> Logistic Regression: 0.947  
> K Nearest Neighbors: 0.930  
> Classification Tree: 0.930

> #instantiate a VotingClassifier 'vc'  
> vc = VotingClassifier(estimators = classifiers)  
> #fit 'vc' to the traing set  
> vc.fit(X_train, y_train)  
> y_pred = vc.predict(X_test)  
> print('voting classifier: {.3f}'.format(accuracy_score(y_test, y_pred)))  
> [out] Voting Classifier: 0.953