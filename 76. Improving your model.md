# Pipelines, feature & text preprocessing
## The pipline workflow
- repeatable way to go from raw data to trained model
- pipeline object takes sequential list of steps
  - output of one step is input to next step
- each step is a tuple with two elements
  - Name: string
  - transform: obj implementing .fit() and transform()
- flexible: a step can itself be another pipline!

__instantiate simple pipeline with one step__
> from sklearn.pipeline import Pipeline  
> from sklearn.linear_model import LogisticRegression  
> from sklearn.multiclass import OneVsRestClassifier  
> pl = Pipeline([('clf', OneVsRestClassifier(LogisticRegression()))])

__train and test with sample numeric data__
> from sklearn.model_selection import train_test_split  
> X_train, X_test, y_train, y_test = train_test_split(sample_df[['numeric]], pd.get_dummies(sample_df['label'], random_state = 2))  
> pl.fit(X_train, y_train)  
> accuracy = pl.score(X_test, y_test)  
> print('accuracy on numeric data, no nans:', accuracy)  
> [out] accuracy on numeric data, no nans: 0.44

__adding more steps to the pipeline__
> X_train, X_test, y_train, y_test = train_test_split(sample_df[['numeric', 'with_missing']], pd.get_dummies(sample_df['label']), random_state = 42)  
> pl = Pipline([('imp', Imputer()), ('clf', OneVsRestClassifier(LogisticRegression()))])

__preprocessing numeric features with missing data__
> pipline.fit(X_train, y_train)  
> accuracy = pl.score(X_test, y_test)  
> print('accuracy on all numeric, incl nans:', accuracy)  
> [out] accuracy on all numeric, incl nans: 0.48

# Text features and feature unions
__preprocessing text features__
> from sklearn.feature_extraction.text import CountVectorizer  
> X_train, X_test, y_train, y_test = train_test_split(sample_df['text], pd.get_dummies(sample_df['label']), random_state = 2)  
> pl = Pipeline([('vec', CountVectorizer()), ('clf', OneVsRestClassifier(LogisticRegression()))])  
> pl.fit(X_train, y_train)  
> pl.score(X_test, y_test)

# Preprocessing multiple dtypes
- want to use __all__ available features in one pipeline
- problem
  - Pipeline steps for numeric and text preprocessing can't follow each other
  - e.g. output of CountVectorizer can't be input to Imputer
- solution
  - FunctionTransformer() & FeatureUnion()

## Function Transformer
- turns a Python function into an object that a scikit-learn pipeline can understand
- need to write two functions for pipeline preprocessing
  - take entire DataFrame, return numeric columns
  - take entire DataFrame, return text columns
- can then preprocess numeric and text data in separate pipelines

__putting it all together__
> X_train, X_test, y_train, y_test = train_test_split(sample_df[['numeric', 'with_missing', 'text]], pd.get_dummies(sample_df['label']), random_state = 2)  
> from sklearn.preprocessing import FunctionTransformer  
> from sklearn.pipeline import FeatureUnion  
> get_text_data = FunctionTransformer(lambda x :['text', validate = False])  
> get_numeric_data = FunctionTransformer(lambda x : x[['numeric', 'with_missing']], validate = False)

__featureUnion Text and Numeric Features__
> from sklearn.pipeline import FeatureUnion  
> union = FeatureUnion([('numeric', numeric_pipeline),('text', text_pipeline)])

__putting it all together__
> numeric_pipeline = Pipeline([('selector', get_numeric_data), ('imputer', Imputer())])  
> text_pipeline = Pipeline([('union', FeatureUnion([('numeric', numeric_pipeline)('text', text_pipeline)])), ('clf', OneVsRestClassifier(LogisticRegression()))])

# Choosing a classification model
__main dataset: lots of text__
> LABELS = ['Function', 'Use', 'Sharing', 'Reporting', 'Student_Type', 'Position_Type', 'Object_Type', 'Pre_K', 'Operating_Status']  
> NON_LABELS = [c for c in df.columns if c not in LABELS]  
> len(NON_LABELS) - len(NUMERIC_COLUMNS)  
> [out] 14

__using pipeline with the main dataset__
> import numpy as np  
> import pandas as pd  
> df = pd.read_csv('TrainingSetSample.csv', index_col = 0)  
> dummy_labels = pd.get_dummies(df[LABELS])  
> X_train, X_test, y_train, y_test = multilabel_train_test_split(df[NON_LABELS], dummy_labels, 0.2)

> get_text_data = FunctionTransformer(combine_text_columns, validate = False)  
> get_numeric_data = FunctionTransformer(lambda x: x[NUMERIC_COLUMNS], validate = False)  
> pl = Pipeline([('union', featureUnion([('numeric_features', Pipeline([('selector', get_numeric_data), ('imputer', Imputer())]), ('text_features', Pipeline([('selector', get_text_data), ('vectorizer', CountVectorizer())])))]), ('clf', OneVsRestClassifier(LogisticRegression())))])

__performance using main dataset__
> pl.fit(X_train, y_train)  

## Flexibility of model step
- is current model the best?
- can quickly try different models with pipelines
  - pipeline preprocessing steps unchanged
  - edit the model step in your pipeline
  - random forest, Naive Bayes, k-NN

__using CountVectorizer() on column of main dataset__
> vec_basic.fit(df.Program_Description)  
> msg = 'there are {} tokens in Program_Description if tokens are any non-whitespace'  
> print(msg.format(len(vec_basic.get_feature_names())))  
> [out] there are 157 tokens in Program_Description if tokens are any non-whitespace

__easily try new models using pipeline__
> from sklearn.ensemble import RandomForestClassifier  
> pl = Pipeline([('union', FeatureUnion(transformer_list = [('numeric_features', Pipeline([('selector', get_numeric_data), ('imputer', Imputer())])),('text_features', Pipeline([('selector', get_text_data), ('vectorizer', CountVecorizer())]))])), ('clf', OneVsRest(__RandomForestClassifier__()))])