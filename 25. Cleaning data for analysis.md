# Data Types

__object type is usually string__

__to see the data type__
>print(df.dtypes)
>print(df.info())
## Converting data types
__to string__
> df[column_name] = df[column_name].astype(str)

__to category__

> df[column_name] = df[column_name].astype('category)

## Categorical data
- converting categorical data to 'category' dtype:
  - can make the DataFrame smaller in memory
  - can make them be utilized by other Python libraries for analysis

NOTE : if numeric data loaded as a string/object, __not int__, this is a sign of __bad data contained.

## Cleaning bad data : [pandas to_numeric](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_numeric.html)

### __errors__
- 'raise' : the missing value will raise exception
- 'coerce' : the missing value will be set as NaN
- 'ignore' : the missing value will return input

__to_numeric__
> df[column] = pd.to_numeric(df[column], errors = 'coerce')

# Using regular expressions to clean strings

## String manipulation
- many bulit-in and external libraries
- __re__ library for regular expression
  - a formal way of specifying a pattern
  - sequence of characters
- pattern matching
  - similar to globbing

example :
> 17 = \d*  
> $17 = \$\d*  
> 17.00 = \$\d*\.\d*  
> 17.00 = \$\d*\.\d{2} (to specify decimal place)  
> Hello = [A-Z]\w*
> 

__NOTE__ ^ = beginning, $ = ending

## Using regular expressions
1. compile the pattern
2. use th ecompiled pattern to match values
3. this let us use the pattern over and over again
4. useful since we want to match values down a column of values

> import re  
> pattern = re.compile('match style')  
> result = pattern.match('match object')
> result = True or False

__re.findall()__
>re.findall('pattern', 'string')
>print(result)

# Use functions to clean data

- cleaning step requires multiple steps
  - extract number from string
  - perform transformation on extracted number
- Python function

NOTE : axis = 0 means vertically(default up and down)  
axis = 1 means horizontally

## __apply__
__column mean__
> df.apply(np.mean, axis = 0)

__row mean__
>df.apply(np.mean, axis = 1)

DataSet:

|ID |column1|column2|column3
|---|---|---|--|
|01|$123|$987|
|02|$456|$654|
|03|$789|$321|

__we want to__
1. remove the dollar sign
2. subtract column1 and column2 and saved result to new column

## write the regular expression  
> import re  
> from numpy import NaN  
> pattern = re.compile('pattern')

## write the function

> def function(row,pattern):  
> var1 = row['column1']  
> var2 = row['column2']  
> if bool(pattern.match(var1)) and bool(pattern.match(var2)) :  
> var1 = var1.replace(' $ ','')  
> var2 = var2.replace(' $ ', '')  
> var1 = float(var1)  
> var2 = float(var2)  
> return var1 - var2
> else:
> return(NaN)

>df_subset['column3'] = df_subset.apply(function, axis = 1, pattern = pattern)

NOTE : combine lambda function
> data[column] = data.column.apply(lambda x : x.replace('$','')

# Duplicate and missing data

## __drop_duplicates( ) method__
>df = df.drop_duplicates( )

dealing with missing data
- leave as-is
- drop them 
  - drop_nan.data
- fill missing value 
  - data.fillna()

##  __check missing values__
>data_nan.info()

## __drop missing values__
>data_nan.dropna()

## __fill missing values__
- fill with provided value
- use a summary statistic

__provided value__
>data['column'] = data['column'].fillna('missing')
>data[[column1,column2]] = data[[column1,column2]].fillna(0)

fill missing values with a test statistic
- careful when using test statistics to fill
- have to make sure the value you are filling in makes sense
- median is a better statistic in the presence of outliers  

__mean value__
>mean = data[column].mean()
>data[column] = data[column].fillna(mean)

# Testing with asserts
- programmatically vs visually checking
- if we drop or fill NaNs, we expect 0 missing values
- we can write an assert statement to verify this 
- we can detect early warning and errors
- this gives us confidence that our code is running correctly


## Assert
- assert truth will return nothing
- assert false will return error

> assert 1 == 1  
> [Output]  
> assert 1 == 2  
> [Output]AssertionError

>assert data.column.notnull().all()
