# It's time to build a model
- always a good approach to start with a very simple model
- gives a sense of how challenging the problem is
- many more things can go wrong in complex models
- how much signal can we pull out using basic methods

<br>

- train basic model on numeric data only
  - want to go from raw data to predictions quickly
- multi-class logistic regression
  - train classifier on each label separately and use those to predict
- format predictions and save to csv
- compute log loss score

## Splitting the multi-class dataset
- recall: train-test split
  - will not work here
  - may end up with labels in test set that never appear in training set
- solution: stratifiedShuffleSplit
  - only works with a single target variable
  - we have many target variables
  - multilabel_train_test_split()

__splitting the data__
> data_to_train = df[NUMERIC_COLUMNS].fillna(-1000)  
> labels_to_use = df.get_dummies(df[LABELS])  
> X_train, X_test, y_train, y_test = multilabel_train_test_split(data_to_train, labels_to_use, size = 0.2, seed = 123)  

__training the model__
> from sklearn.llinear_model import LogisticRegression  
> from sklearn.multiclass import OneVsRestClassifier  
> clf = OneVsRestClassifier(LogisticRegression())  
> clf.fit(X_train, y_train)

- OneVsRestClassifier:
  - treat each column of y independently
  - fit a separate classifier for each of the columns

# Making predictions
__predicting on holdout data__
> holdout = pd.read_csv('HoldOutData.csv', index_col = 0)  
> holdout = holdout[NUMERIC_COLUMNS].fillna(-1000)  
> predictions = clf.predict_proba(holdout)

- if .predict() was used instead:
  - output would be 0 or 1
  - lag loss penalizes being confident and wrong
  - worse performance compared to .predict_proba
  
## submitting your predictions as a csv
- all formatting can be done with the pandas to_csv function

> prediction_df = pd.DataFrame(columns = pd.get_dummies(df[LABELS], prefix_sep = '__'.columns, index = holdout.index, data = predictions))  
> prediction_df.to_csv('predictions.csv')  
> score = score_submission(pred_path = 'predictions.csv')

# A very brief introduction to NLP
- data for NLP:
  - text, documents, speech...
- tokenization
  - splitting a string into segments
  - store segments as list
- example: 'Natural Language Processing'
  - -> ['Natural', 'Language', Processing']

## Tokens and token patterns
- tokenize on whitespace
  - PETRO-VEND FUEL AND FLUIDS
  - PETRO-VEND | FUEL | AND | FLUIDS
- tokenize on whitespace and punctuation
  - PETRO-VEND FUEL AND FLUIDS
    - PETRO | VEND | FUEL | AND | FLUIDS

## Bags of words representation
- count the number of times a particular token appears
- 'bag of words'
  - count the number of times a word was pulled out of the bag
- this approach discards information about word order
  - 'red, not blue' is the same as 'blue, not red'
- n-grams

# Representing text numerically
- bag of words
  - simple way to represent text in machine learning
  - discards information about grammar and word order computes frequency of occurrence

## Scikit-learn tools for bag-of-words
- CountVectorizer()
  - tokenizes all the strings
  - builds a 'vocabulary'
  - counts the occurrences of each token in the vocabulary

__using CountVectorizer() on column of main dataset__
> from skleran.feature_extraction.text import CountVectorizer  
> TOKENS_BSCIS = '\\S + (?=\\s+)'  
> df.Program_Description.fillna('', inplace = True)  
> vec_basic = CountVectorizer(token_patter = TOKENS_BASIC)

> vec_basic.fit(df.Program_Description)  
> msg = 'There are {} tokens in Program_Description if tokens are any non-white'  
> print(msg.format(len(vec_basic/get_feature_names())))  
> [out] Threre are 157 tokens in Program_Description if tokens are any non-whitespace