# How good is your model?
## Classification metrics
- measuring model performance with accuracy:
  - fraction of correctly classified samples
  - not always a useful metric
## Class imbalance example: Emails
- spam classification
  - 99% of emails are real; 1% of emails are spam
- could build a classifier that predicts ALL emails are real
  - 99% accurate!
  - but horrible at actually classifying spam
  - fails at its original purpose
- need more nuanced metrics

## confusion matrix
- true positive: true and predict true(HIT)
- false possitive: false and predict true(false alarm)
- false negative: true and predict false(miss)
- true negative: false and predict false(HIT)

- 準確率accuracy = $\frac{true positive + true negative}{all}$ 正確辨識率
- 精確率precision = $\frac{true positive}{true positive + false positive}$ 判斷為真時的正確率
  - __also call__ positive predictive value, PPV

- 召回率recall = $\frac{true positive}{true positive + false negative}$ 為真時的正確辨識率
  - __also call__ hit rate, sensitivity, true positive rate

- F1 score = $2 * \frac{precision * recall}{precision + recall}$
  - __harmonic mean__ of precision and recall
- high precision: not many real emails predicted as spam
- high recall: predict most spam emails correctly
- class of interest = positive class

__confusion matrix in scikit-learn__
> from sklearn.metrics import classification_report  
> from sklearn.metrics import confusion matrix  
> knn = KNeighborsClassifier(n_neighbors = 8)  
> X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state = 42)  
> knn.fit(X_train, y_train)  
> y_pred = knn.predict(X_test)
> print(confusion_matrix(y_test, y_pred))

# Lofistic regression and the ROC curve
## Lofistic regression for binary classification
- logistic regression outputs probabilities
- if the probability 'p' is greater than 0.5:
  - the data is labeled '1'
- if the probability 'p' is less than 0.5:
  - the data is labeled '0'

__Logistic regression in scikit-learn__
> from sklearn.linear_model import LogisticRegression  
> from sklearn.model_selection import train_test_split  
> logreg = LogisticRegression()  
> X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state = 42)  
> logreg.fit(X_train, y_train)  
> y_pred = logreg.predict(X_test)

## Probability thresholds
- by default, logistic regression threshold = 0.5
- not specific to logistic regression
  - k-NN classifiers also have thresholds
- what happens if we vary the threshold?
  - the ROC curve
  
__plotting the ROC curve,receiver operating characteristic curve__
> from sklearn.metrics import roc_curve  
> y_pred.prob = logreg.predict_proba(X_test)[:,1]  
> fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)  
> plt.plot([0,1], [0,1], 'k---')  
> plt.plot(fpr, tpr, label = 'logistic regression')  
> plt.xlabel('false positive rate')  
> plt.ylabel('true positive rate')  
> plt.title('logistic regression ROC curve')  
> plt.show()

__note__
- confusion_matrix(y_test, y_pred)
- classification_report(y_test, y_pred)
- roc_cruve(y_test, y_pred_proba)

# Area under the ROC curve
- larger area under the ROC curve = better model

__AUC in scikit-learn__
> from sklearn.metrics import roc_auc_score  
> logreg = LogisticRegression()  
> X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.4, random_state = 42)  
> logreg.fit(X_train, y_train)  
> y_pred_prob = logreg.predict_proba(X_test)[:,1]  
> roc_auc_score(y_test, y_pred_prob)  
> 0.997

__AUC using cross-validation__
> from sklearn.model_selection import cross_val_score  
> cv_score = cross_val_score(logreg, X, y, cv = 5, scoring = 'roc_auc')  
> print(cv_score)  
> [out] 0.996 0.991 0.995 1 0.961

# Hyperparameter tuning
- linear regression: choosing parameters
- ridge/lasso regression: choosing alpha
- k-Nearest Neighbors: choosing n_neighbors
- parameters like alpha and k: Hyperparameters
- Hyperparameters connot be learned by fitting the model

##  choosing the correct hyperparameter
- try a bunch of different hyperparameter values
- fit all of them separately
- see how well each performs 
- choose the best performing one
-  it is essential to use cross-validation

__GridSearchCV in scikit-learn__
> from sklearn.model_selection import GridSearchCV  
> param_grid = {'n_neighbors': np.arange(1,50)}  
> knn = KNeighborsClassifier()  
> knn_cv = GridSearchCV(knn, param_grid, cv = 5)  
> knn_cv.fit(X,y )  
> knn_cv.best_params_  
> [out] {'n_neighbors': 12}  
> knn_cv.best_score_  
> [out] 0.933

__RandomizedSearchCV in scikit-learn__
> param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 9),
              "min_samples_leaf": randint(1, 9),
              "criterion": ["gini", "entropy"]}  
> tree = DecisionTreeClassifier()  
> tree_cv = RandomizedSearchCV(tree, param_dist, cv=5)  
> tree_cv.fit(X,y)

# Hold-out set for final evaluation
## hold-out set reasoning
- how well can the model perform on never before seen data?
- using ALL data for cross-validation is not ideal
- split data into training and hold-out set at the beginning
- perform grid search cross-validation on training set
- choose best hyperparameters and evaluate on hold-out set