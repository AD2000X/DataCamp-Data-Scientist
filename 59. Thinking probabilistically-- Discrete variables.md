# Probabilistic Logic and Statistical Inference
## Statistical inference
- To draw probabilistic conclusions about what we might expect if we collected the same data again.
- To draw actionable conclusions from data.
- To draw more general conclusions from relatively few data or observations.
## Using  probabilistic language in statistical inference
- Probability provides a measure of uncertainty.
- Data are almost never exactly the same when acquired again, and probability allows us to say how much we expect them to vary.

# Random number generators and hacker statistics
## Hacker statistics
- use simulated repeated measurements to compute probabilities.

## the np.random module
- suite of functions based on random number generation
- np.random.random() : 
  - draw a number between 0 and 1

## Bernoulli trial
- an experiment that has two options :
  - success(True)
  - failure(False)

## Random number seed
- integer fed into random number generating algorithm
- manually seed random number generator if you need reproducibility
- specified using np.random.seed()

__simulating 4 coin flips__
> import numpy as np  
> np.random.seed(42)  
> random_number = np.random.random(size = 4)  
> random_numbers  
> [out] array([0.37, 0.95, 0.73, 0.59])  
> head = random_number < 0.5  
> heads  
> [out] array([True, False, False, False])  
> np.sum(heads)  
> [out] 1

__simulating 4 coin flips__
> n_all_heads = 0 #initialize number of 4-head trials  
> for _ in range(10000):  
> heads = np.random.random(size = 4) < 0.5  
> n_heads = np.sum(heads)  
> if n_head == 4:  
> n_all_head += 1  
> n_all_heads/10000  
> [out] 0.0621

## Hacker stats probabilities  
- determine how to simulate data  
- simulate many many times  
- probability is approximately fraction of trials with the outcome of interest

# Probability distributions and stories: the binomial distribution
## Probability mass function (PMF)
- the set of probabilities of discrete outcomes

## Probability distribution
- a mathematical description of outcomes

## Discrete uniform distribution: the story
- the outcome of rolling a single fair dice. Discrete Uniformly distributed.

## Binomial distribution: the sotry
- the number r of success in n Bernoulli trials with probability p of success, is Binomially distributed.
- the number r of heads in 4 coin flips with probability 0.5 of heads, is Binomilally distributed.

__sampling from the Binomial distribution__
> np.random.binomial(4, 0.5)# number of trial, posibility of success  
> [out]2  
> np.random.binomial(4, 0.5, size = 10)  
> array([4,3,2,1,1,0,3,2,3,0])

__the binomial PMF__
> samples = np.random.binomial(60, 0.1, size = 1000)

__the Binomial CDF__
> import matplotlib.pyplot as plt  
> import seaborn as sns  
> sns.set()  
> x, y = ecdf(samples)  
> _ = plt.plot(x,y, marker = '.', linestyle = 'none')  
> plt.margins(0.02)  
> _ = plt.xlabel('number of successes')  
> _ = plt.ylabel('CDF')  
> plt.show()

# Poisson processes and the Poisson distribution
## Poisson process
- the timing of the next event is completely independent of when the previous event happened.

## examples of Poisson processes
- natural births in a given hospital
- hit on a website during a given hour
- meteor strikes
- molecular collisions in a gas
- aviation incident
- buses in Poissonville

## Poisson distribution
- the number r of arrivals of a Poisson process in a given time interval with average rate of l arrivals per interval is Poisson distributed
- the number r of hits on a website in one hours with an average hit rate of 6 hits per hour is Poisson distributed.
- limit of the binomial distribution for low probability of success and large number of trials
- that is, for rare events.

__the Poisson CDF__
> samples = np.random.poisson(0, size = 10000)# mean and size  
> x, y = ecdf(samples)  
> _ = plt.plot(x, y, marker = '.', linestyle = 'none')  
> plt.margins(0.02)  
> _ = plt.xlabel('number of successes')  
> _ = plt.ylabel('CDF')  
> plt.show()