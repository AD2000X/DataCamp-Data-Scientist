# Unsupervised learning
- unsupervised learning finds patterns in data
- e.g. clustering customers by their purchases
- compressing the data using purchase patterns(dimension reduction)

## Supervised vs unsupervised learning
- supervised learning finds patterns for a prediction task
- e.g. classify tumors as benign or cancerous(labels)
- unsupervised learning finds patterns in data
- ...but without a specific prediction task in mind

## iris dataset
- measurements of many iris plants
- 3 species of iris: setosa, versicolor, virginica
- petal length, petal width, sepal length, sepal width(the features of the dataset)

## arrays, features & samples
- 2D Numpy array
- columns are measurements(the features)
- rows represent iris plants(the samples)

## iris data is 4-dimensional
- iris samples are points in 4 dimensional space
- dimension = number of features
- dimension too high to visualize!
- ... but unsupervised learning gives insight

## k-means clustering
- finds clusters of samples
- number of clusters must be specified
- implemented in sklearn('scikit-learn')

__k-mean clustering with scikit-learn__
> print(samples)  
> from sklearn.cluster import KMeans  
> model = KMeans(n_clusters = 3)  
> model.fit(samples)  
> labels = model.predict(samples)  
> print(labels)  
> [out] [0 0 1 1 0 1 2 1 0 1]

__cluster labels for new samples__
- new sample can be assigned to existing clusters
- k-means remember the mean of each cluster(the 'centroids')
- find the nearest centroid to each new sample

__cluster labels for new samples__
> print(new_samples)  
> new_labels = model.predict(new_samples)  
> print(new_labels)  
> [out] [0 2 1]

__scatter plots__
- scatter plot of sepal length vs petal length
- each point represents an iris sample
- color points by cluster labels
- PyPlot(matplotlib.pyplot)

__scatter plots__
> import matplotlib.pyplot as plt  
> xs = samples[:,0]  
> ys = samples[:,2]  
> plt.scatter(xs, ys, c = labels)  
> plt.show()

# Evaluating a clustering
- can check correspondence with e.g. iris species
- ... but what if there are no species to check against?
- measure quality of a clustering
- informs choice of how many clusters to look for

## Iris: clusters vs species
- k-means found 3 clusters amongst the iris samples
- do the clusters correspond to the species?

## Cross tabulation with pandas
- clusters vs species is a 'cross-tabulation'
- use the pandas library
- given the species of each sample as a list species

> print(species)  
> [out] ['setosa', 'setosa', 'versicolor', 'virginica', ...]  

__aligning labels and species__
> import pandas as pd  
> df = pd.DataFrame({'labels' : labels, 'species' : species})

__crosstab of labels and species__
> ct = pd.crosstab(df['labels'], df['species'])

how to evaluate a clustering, if there were no species information?

## Measuring clustering quality
- using only samples and their cluster labels
- a good clustering has tight clusters
- ... and samples in each cluster bunched together

## Inertia measures clustering quality
- measures how spread out the cluster are (lower is better)
- distance from each sample to centroid of its cluster
- after fit(), available as attribute inertia_  
- k-means attempts to minimize the inertia when choosing clusters

> from sklearn.cluster import KMean  
> model = KMean(n_clusters = 3)  
> model.fit(samples)  
> print(model.inertia_)  
> [out] 78.94

## The number of clusters
- clusterings of the iris dataset with different numbers of clusters
- more clusters means lower inertia
- what is the best number of clusters?

## How many clusters to choose?
- a good clustering has tight cluster(so low inertia)
- ... but not too many clusters!
- choose an 'elbow' in the inertia plot
- where inertia begins to decrease more slowly
- e.g. for iris dataset 3 is a good choice

NOTE:
> labels = model.fit_predict(samples) = model.fit().predict()

# Transforming features for better clusterings
## Piedmont wines dataset
- 178 samples from 3 distinct varieties of red wind: Barolo, Grignolino and Barbera
- features measure chemical composition e.g. alcohol content
- ... also visual properties like 'color intensity'

__clustering the wines__
> from sklearn.cluster import KMeans  
> model = KMeans(n_clusters = 3)  
> labels = model.fit_predict(samples)

__clusters vs. varieties__
> df = pd.DataFrame({'labels' : labels, 'varieties' : varieties})  
> ct = pd.crosstab(df['labels'], df['varieties'])  

## Feature variances
- the wine features have very different variances!
- variance of a feature measures spread of its values

__StandardScaler__
- in kmeans: feature variance = feature influence
- StandardScalar transforms each feature to have mean 0 and variance 1
- features are said to be 'standardized'

__sklearn StandardScaler__
> from sklearn.preprocessing import StandardScaler  
> scaler = StandardScaler()  
> scaler.fit(samples)  
> samples_scaled = scaler.transform(samples)  

## Similar methods
- StandardScaler and KMeans have similar methods
- use fit()/ transform() with StandardScaler
- use fit()/ predict() with KMeans

## StandardScaler, then KMeans
- need to perform two steps: StandardScaler, then KMeans
- use sklearn pipeline to combine multiple steps
- data flows from one step into the next

__pipeline combine multiple steps__
> from sklearn.preprocessing import StandardScaler  
> from sklearn.cluster import KMeans  
> scaler = StandardScaler()  
> kmeans = KMeans(n_clusters = 3)  
> from sklearn.pipeline import make_pipeline  
> pipeline = make_pipeline(scaler, kmeans)  
> pipeline.fit(samples)  
> labels = pipeline.predict(samples)

__feature standardization improves clustering__
> df = pd.DataFrame({'labels':labels, 'varieties':varieties})  
> ct = pd.crosstab(df['labels'], df['varieties'])

## sklearn preprocessing steps
- StandardScaler is a 'preprocessing' step
- MaxAbsScaler and Normalizer are other examples