# Learning from the expert: processing
- text processing
- statistical method
- computational efficiency

## learning from the expert: text preprocessing
- NLP tricks for text data
  - tokenize on punctuation to avoid hyphens, underscores, etc.
  - include unigrams and bi-grams in the model to capture important information involving multiple tokens- e.g. 'middle school'

__N-grams and tokenization__
> vec = CountVectorizer(token_pattern = TOKEN_ALPHANUMERIC, ngram_range = (1,2))
- simple changes to CountVectorizer
  - alphanumeric tokenization
  - ngram_range = (1,2)

__range of n-grams in scikit-learn__
> pl.fit(X_train, y_train)  
> holdout = pd.read_csv('HoldoutData.csv', index_col = 0)  
> predictions = pl.predict_proba(holdout)  
> prediction_df = pd.DataFrame(columns = pd.get_dummies(df[LABELS]).columnsm index = holdout.index, data = prediction)  
> prediction_df.to_csv('predictions.csv')  
> score = score_submission(pred_path = 'predictions.csv')

# Learning from the expert: a stats trick
## Learning from the expert: interaction terms
- statistical tool that the winner used: interaction terms
- Example:
  - english teacher for 2nd grade
  - 2nd grade - budget for english teacher
- interaction terms mathematically describe when tokens appear together

__adding interaction features with scikit-learn__
> from sklearn.preprocessing import PolynomialFeatures  
> x  
> [out]   
>&nbsp;&nbsp;&nbsp; x1 x2  
> a&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp; 1  
> b&nbsp;&nbsp;&nbsp; 1&nbsp;&nbsp; 1  
> interaction = PolynomialFeatures(degree = 2, interation_only = True, include_bias = False)  
> interaction.fit_transform(x)  
> [out] array([[0, 1, 0],  
> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[1, 1, 1]])

## a note about bias terms
- bias term allows model to have non-zero y value when x value is zero

__sparse interaction features__
> SparseInteractions(degree = 2).fit_transform(x).toarray()  
> [out] array([[0, 1, 0],  
> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[1, 1, 1]])
- the number of interaction terms grows exponentially
- our vectorizer saves memory by using a sparse matrix
- PolynomialFeatures does not support sparse matrices
- we have provided SparseInteractions to work for this problem

# Learning from the expert: a computational trick and the winning model
## Learning from the expert: hashing trick
- adding new features may cause enormous increase in array size
- hashing is a way of increasing memory efficiency
- hash function linmits possible outputs, fixing array size

## when to use the hashing trick
- want to make array of features as small as possible
  - dimensionality reduction
- particularly useful on large datasets
  - e.g. lots of text data!

__implementing the hashing trick in scikit-learn__
> from sklearn.feature_extraction.text import HashingVectorizer  
> vec = HashingVectorizer(norm = None, non_negative = True, token_pattern = TOKENS_ALPHANUMERIC, ngram_range = (1,2))

## The model that won it all
- you now know all the expert moves to make on this dataset
  - NLP: range of n-grams, punctuation tokenization
  - stats: interaction terms
  - computation: hashing trick
- what class of model was used? And the winning model was...
- logistic regression!
  - carefully create features
  - easily implemented tricks
- Favor simplicity over complexity and see how far it takes you!

# Next steps and the social impact of your work
## can you do better?
- you've seen the flexivility of the pipeline steps
- quickly test ways of improving your submission
  - NLP: stemming, stop-word removal
  - model: RandomForest, k-NN, Naive Bayes
  - Numeric Preprocessing: Imputation strategies
  - Optimization: Grid search over pipeline objects
  - Experiment with new scikit-learn techniques
- work with the full dataset at DrivenData!

__Hundreds of hours saved__
- make schools more efficient by improving their budgeting decisions
- save hundreds of hours each year that humans spent labeling line items
- can spend more time on the decisions that really matter

## DrivenData: data science to save the world
- other ways to use data science to have a social impact at www.drivendata.org
  - imporve your data science skills while helping meaningful organizations thrive
  - win some cash prizes while you're at it!
  - 