# Preprocessing data
## Dealing with categorical features
- scikit-learn will not accept categorical features by default
- need to encode categorical features numerically
- convert to 'dummy variables'
  - 0: observation was NOT that category
  - 1: observation was that category
## Dealing with categorical features in Python
- scikit-learn: OneHotEncoder()
- pandas: get_dummies()

## Automobile dataset
- mpg: target variable
- origin: categorical feature

__encoding dummy variables__
> import pandas as pd  
> df = pd.read_csv('auto.csv')  
> df_origin = pd.get_dummies(df)
> [out] dummy the categorical variable  
> df_origin = df_origin.drop('origin_Asia', axis = 1)

> df_origin = pd.get_dummies(df, drop_first = True)

__linear regression with dummy variables__
> from sklearn.model_selection import train_test_split  
> from sklearn.linear_model import Ridge  
> X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 42)  
> ridge = Ridge(alpha = 0.5, normalize = True).fit(X_train, y_train)  
> ridge.score(X_test, y_test)  
> [out] 0.719

# Handling missing data
__dropping missing data__
> df.insulin.replace(0, np.nan, inplace = True)  
> df.triceps.replace(0, np.nan, inplace = True)  
> df.bmi.replace(0, np.nan, inplace = True)  

__dropping missing data__
> df = df.dropna()

__imputing missing data__
- making an educated guess about the missing values
- example: Using the mean tof the non-missing entries
> from sklearn.preprocessing import Imputer  
> imp = Imputer(missing_values = 'NaN', strategy = 'mean', axis = 0)  
> imp.fit(X)  
> X = imp.transform(X)  

__imputing within a pipeline__
> from sklearn.pipline import Pipline  
> from sklearn.preprocessing import Imputer  
> imp = Imputer(missing_values = 'NaN', strategy = 'mean', axis = 0)  
> logreg = LogisticRegression()  
> steps = [('imputation', imp), ('logistic_regression', logreg)]  
> pipeline = Pipeline(steps)  
> X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)  

> pipeline.fit(X_train, y_train)  
> y_pred = pipeline.predict(X_test)  
> pipeline.score(X_test, y_test)  
> [out] 0.753

NOTE:
- number of NaN: df.isnull().sum()
- dropna and fillna method

# Centering and scaling
## Why scale your data?
- many models use some form of distance to inform them
- features on larger scales can unduly influence predictions
- we want feature to be on a similar scale
- normalizing(or scaling and centering)

## Way to normalize your data
- standardization: subtract the mean and divide by variance
  - all features are centered around zero and have variance one
- can also subtract the minimum and divide by the range
  - minimum zero and maximum one
- can also normalize so the data ranges from -1 to +1
- see scikit-learn docs for furthers details

__scaling in scikit-learn__
> from sklearn.preprocessing import scale  
> X_scaled = scale(X)  
> np.mean(X), np.std(X)  
> [out] 8.134, 16.72  
> np.mean(X_scaled), np.std(X_scaled)  
> [out] 2.54e-15, 1.0

__scaling in a pipeline__
> from sklearn.preprocessing import StandardScaler  
> steps = [('scaler', StandardScaler()), ('knn', KNeighborClassifier())]  
> pipeline = Pipeline(steps)  
> X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 21)  
> knn_scaled = pipeline.fit(X_train, y_train)  
> y_pred = pipeline.predict(X_test)  
> accuracy_score(y_test, y_pred)  
> [out] 0.956  
> knn_unscaled = KNeighborsClassifier().fit(X_train,y_train)  
> knn_unscaled.score(X_test, y_test)  
> [out] 0.928

__CV and scaling in a pipeline__
> steps = [('scaler', StandardScaler()), ('knn', KNeighnorsClassifier())]  
> pipeline = Pipeline(steps)  
> parameters = {knn__n_neighnors = np.arange(1,50)}  
> X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 21)  
> cv = GridSearchCV(pipeline, param_grid = parameters)  
> cv.fit(X_train, y_train)  
> y_pred = cv.predict(X_test)

> print(cv.best_params_)  
> [out] knn__n_neighbors:41  
> print(cv.score(X_test, y_test))  
> 0.956  
> print(classification_report(y_test, y_pred))

# Final thoughts
## what you've learned
- using machine learning techniques to build predictive models
  -  for both regression and classification problems
  -  with real-world data
- underfitting and overfitting
- test-train split
- cross-validation
- grid search
- regularization, lasso and ridge regression
- data preprocessing
- for more: check out
  - [scikit-learn documentation](https://scikit-learn.org/stable/documentation.html)
  - [introduction to machine learning with Python](https://github.com/amueller/introduction_to_ml_with_python)