# Visualizing the PCA transformation
## Dimension reduction
- more efficient storage and computation
- remove less-informative 'noise' feature
- ...which cause problems for prediction tasks, e.g. classification, regression

## Principal Component Analysis
- PCA = 'Principal Component Analysis'
- fundamental dimension reduction technique
- first step 'decorrelation' (considered here)
- second step, reduces dimension(considered later)

## PCA aligns data with axes
- rotates data samples to be aligned with axes
- shifts data samples so they have mean 0
- no information is lost

## PCA follows the fit/transform pattern
- __PCA__ a scikit-learn component like __KMeans__ or __StandardScaler__
- fit() learns the transformation from given data
- transform() applies the learned transformation
- transform() can also be applied to new data

## Using scikit-learn PCA
- samples = array of two wine features ( total_phenols & od280)
> print(samples)  
> [out] [[2.8 3.92]  
> [2.65 4.3]  
> ...]  
> from sklearn.decomposition import PCA  
> model = PCA()  
> model.fit(samples)  
> [out] PCA(copy = True,...)  
> transformed = model.transform(samples)

## PCA features
- row of transformed correspond to samples
- columns of transformed are the 'PCA features'
- row gives PCA feature values of corresponding sample
> print(transformed)  
> [out] [1.32 ...]

## PCA features are not correlated
- features of dataset are often correlated, e.g. total_phenols and od289
- PCA aligns the data with axes
- resulting PCA features are not linearly correlated('decorrelation')

## Pearson correlation
- measures linear correlation of features
- value between -1 and 1
- value of 0 means no linear correlation

## Principal components
- 'principal components' = directions of variance
- PCA aligns principal components with axes
- available as components_ attribute of PCA object
- each row defines displacement from mean
> print(model.components_)  
> [out] [[0.64 0.76]  
> [-0.76 0.64]]

# Intrinsic Dimension
## Intrinsic dimension of a flight path
- 2 features: longitude and latitude at points along a flight path
- dataset appears to be 2-dimensional
- but can approximate using one features: displacement along flight path
- is intrinsically 1-dimensional

## Intrinsic dimension
- intrinsic dimension = number of features needed to approximate the dataset
- essential idea behind dimension reduction
- waht is the most compact representation of the samples?
- can be detected with PCA

## Versicolor dataset
- 'versicolor', one of the iris species
- only 3 features: sepal length, sepal width, and petal width
- samples are points in 3D space

## Versicolor dataset has intrinsic dimension 2
- samples lie close to a flat 2-dimensional sheet
- so can be approximated using 2 features

## PCA indentifies intrinsic dimension
- scatter plots work only if samples have 2 or 3 features
- PCA identifies intrinsic dimension when samples have any number of features
- intrinsic dimension = number of PCA features with significant variance

## Variance and intrinsic dimension
- intrinsic dimension is number of PCA features with significant variance
- in out example the first two PCA features
- so intrinsic dimension is 2

## Plotting the variance of PCA features
- samples = array of versicolor samples
> import matplotlib.pyplot as plt  
> from sklearn.decomposition import PCA  
> pca = PCA()  
> pca.fit(samples)  
> [out] PCA(copy = True, ...)  
> features = range(pca.n_components_)  

> plt.bar(features, pca.explained_variance_)  
> plt.xticks(features)  
> plt.ylabel('variance')  
> plt.xlabel('PCA feature')  
> plt.show()

## Intrinsic dimension can be ambiguous
- intrinsic dimension is an idealization
- ... there is not always one correct answer!
- piedmont wines: could argue for 2 or for 3 or more

# Dimension reduction with PCA
- represents same data, using less features
- important part of machine-learning pipelines
- can be performed using PCA

<br>

- PCA features are in decreasing order of variance
- assumes the low variance features are 'noise'
- ... and high variance features are informative

<br>

- specify how many features to keep
- e.g. PCA(n_components = 2)
- keeps the first 2 PCA features
- intrinsic dimension is a good choice

## Dimension reduction of iris dataset
- samples = array of iris measurements(4 features)
- species = list of iris species numbers
> from sklearn.decomposition import PCA  
> pac = PCA(n_components = 2)  
> pca.fit(samples)  
> [out] PCA(copy = True, ...)  
> transformed = pca.transform(samples)  
> print(transformed.shape)  
> [out] (150,2)

## Iris dataset in 2 dimensions
- PCA has reduced the dimenstion to 2
- retained the 2 PCA features with highest variance
- important information preserved: species remain distinct
> import matplotlib.pyplot as plt  
> xs = transformed[:,0]  
> ys = transformed[:,1]  
> plt.scatterï¼ˆxs, ys, c = species  
> plt.show()

## Dimension reduction with PCA
- discard low variance PCA features
- assumes the high variance features are informative
- assumption typically holds in practice(e.g. for iris)

## Word frequency arrays
- rows represent documents, columns represent words
- entries measure presence of each word in each document
- ... measure using 'tf-idf'(more later)

## Sparse arrays and csr_matrix
- array is 'sparse': most entries are zero
- can use scipy.sparse.csr_matrix instead of NumPy array
- csr_matrix remembers only the non-zero entries(save space!)

## TruncatedSVD and csr_matrix
- scikit-learn PCA doesn't support csr_matrix
- use scikit-learn TruncatedSVD instead
- performs same transformation
> from sklearn.decomposition import TruncatedSVD  
> model = TruncatedSVD(n_components = 3)  
> model.fit(documents) # documents is csr_matrix  
> [out] TruncatedSVD(algorithm = 'randomized', ...)  
> transformed = model.transform(documents)