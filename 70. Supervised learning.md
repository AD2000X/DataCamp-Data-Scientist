# What is Machine Learning
## The art and science of 
- giving computers the ability to learn to make decisions from data
- without being explicity programmed!
## Examples:
- learning to predict whether an email is spam or nor
- clustering wikipedia entries into different categories
## Supervised learning: uses labeled data
## Unsupervised learning: uses unlabeled data

## Unsupervised learning
- uncovering hidden patterns from unlabeled data
- Example:
  - grouping customers into distince categories(Clustering)

## Reinforcement learning
- software agent interact with an environment
  - learn how to optimize their behavior
  - given a system of rewards and punishments
  - draws inspiration from behavioral psychology
- Application
  - economics
  - genetics
  - game playing

## Supervised learning
- predictor variables/features and a target variable
- aim: predict the target variable, given the predictor variables
  - Classification: Target variable consists of categories
  - Regression: Target variable is continuous

## Naming conventions
- features = predictor variables = independent variables
- target variables = dependent variable = response variable

## Supervised learning
- automate time-consuming or expensive manual tasks
  - example: doctor's diagnosis
- make predictions about the future
  - example: will a customer click on an ad or not?
- Need labeled data
  - historical data with labels
  - experiments to get labeled data
  - crowd-sourcing labeled data

## Supervised learning in python
- we will use scikit-learn/sklearn
  - integrates well with the SciPy stack
- Other libraries
  - TensorFlow
  - keras

# Exploratoy Data Analysis
## The Iris dataset
- features:
  1. petal length
  2. petal width
  3. sepal length
  4. sepal width
- Target variables: Species
  1. versicolor
  2. virginica
  3. setosa

__the iris dataset in scikit-learn__
> from sklearn import datasets  
> import pandas as pd  
> import numpy as np  
> import matplotlib.pyplot as plt  
> plt.style.use('ggplot')  
> iris = datasets.load_iris()  


> type(iris)  
> [out] sklearn.datasets.base.Bunch# bunch is like dict, with key-value pairs  
> print(iris.keys())  
> [out] dict_keys(['data', 'target_names', 'DESCR', 'feature_names', 'target'])  
> type(iris.data), type(iris.target)  
> [out] (numpy.ndarray, numpy.ndarray)  
> iris.data.shape  
> [out] (150,4)  
> iris.target_names  
> [out] array(['setosa', 'versicolor', 'virginica'], dtype = '<U10')

__exploratory data analysis(EDA)__
> x = iris.data  
> y = iris.target  
> df = pd.DataFrame(x, columns = iris.feature_names)  

__visual EDA__
> _ = pd.scatter_matrix(df.color = y, figsize = [8,8], s = 150, marker = 'D')# s = marker size  

__EDA tools__
> df.info()  
> df.describe()  
> df.head()
> 
__visual EDA with countplot__
> plt.figure()# set up new figure  
> sns.countplot(x='education', hue='party', data=df, palette='RdBu')

# The classification challenge
## k-nearest neighbors
- looking at the 'k' closest labeled data points
- taking a majority vote

## Scikit-learn fit and predict, fit() and predict() method
- all machine learning models implemented as Python classes
-  they implement the algorithms for learning and predicting
-  store information learned from the data
- Training a model on the data = 'fitting' a model to the data
  - .fit() method
- to predict the labels of new data: .predict() method

__using scikit-learn to fit a classifier__
> from sklearn.neighbors import KNeighborsClassifier  
> knn = KNeighborsClassifier(n_neighbors = 6)  
> knn.fit(iris['data'], iris['target'])#feather, target

__predicting on unlabeled data__
> prediction = knn.predict(x_new)#x_new is unlabeled data  

# Measuring model performance
- in classification, accuracy is a commonly used metric
- accuracy = fraction of correct predictions
- which data should be used to compute accuracy?
- how well will the model perfrom on new data?
- could compute accuracy on data used to fit classifier
  - NOT indicative of ability to generalize
- split data into training and test set
  - fet/train the classifier on the training set
  - make predictions on test set
  - compare predictions with the known labels

## Train/test split
> from sklearn.model_selection import train_test_split  
> x_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.3, random_state = 21, stratify = y)  
> #順序是訓練-測試，data - label  
> #radom_state = random seed, 4 args = training data, test data, training label, test label   
> knn = KNeighborsClassifier(n_neighbors = 8)  
> knn.fit(x_train, y_train)  
> #用來訓練的 xy  
> y_pred = knn.predict(x_test)  
> #用來測試的 x  
> print(ypred)  
> knn.score(x_test, y_test)  
> #比較 x_test 吐出的 label 跟 y_test 的實際 label  
> [out] 0.9566

## Model Complexity
- large k = smoother decision boundary = less complex model
- smaller k = more complex model = can lead to overfitting