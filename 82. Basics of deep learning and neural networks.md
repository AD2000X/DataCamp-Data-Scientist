# Interactions
- neural networks account for interactions really well
- deep learning uses especially powerful neural networks
  - text
  - images
  - videos
  - audio
  - source code

## Course Structure
- first two chapters focus on conceptual knowledge
  - debug and tune deep learning models on conventional prediction problems
  - lay the foundation for progressing towards modern applications
- this will pay of in the third and foutth chapters

__build deep learning models with keras__
> import numpy as np  
> from keras.layers import Dense  
> from keras.models import Sequential  
> predictors = np.loadtxt('predictors_data.csv', delimiter = ',')  
> n_cols = predictors.shape[1]  
> model = Sequential()  
> model.add(Dense(100, activation = 'relu', input_shape = (n_cols,)))  
> model.add(Dense(100, activation = 'relu'))  
> model.add(Dense(1))

## Deep learning models capture interactions
- interaction in neural network(input layer - hidden layer - output layer)

# Foward propagation
## Bank transactions example
- make predictions based on:
  - number of children
  - number of existing accounts

## Foward propagation
- multiply - add process
- dot product
- foward propagation for one data point at a time
- output is the prediction for that data point

__Forward propagation code__
> import numpy as np  
> input_data = np.array([2,3])  
> weights = {'node_0': np.array([1,1]), 'node_1': np.array([-1,1]), 'output': np.array([2, -1])}  
> node_0_value = (input_data * weights['node_0']).sum()  
> node_1_value = (input_data * weights['node_1']).sum()

> hidden_layer_values = np.array([node_0_value, node_1_value])  
> print(hidden_layer_values)  
> [5,1]  
> output = (hidden_layer_values * weights['output']).sum()  
> print(output)  
> 9

## Activation functions
- applied to node inputs to produce node output
- to capture non-linearities

ReLu(Rectified Linear Activation)
- rectifier

- $RELU(x) = \begin{Bmatrix}
0\;if\;x < 0\\ x\;if\;x >= 0 
\end{Bmatrix}$

__Activation functions__
> import numpy as np  
> input_data = np.array([-1, 2])  
> weights = {'node_0':np.array([3,3]), 'node_1':np.array([1,5]), 'output':np.array([2,-1])}  
> node_0_input = (input_data * weights['node_0']).sum()  
> node_0_output = np.tanh(node_0_input)  
> node_1_input = (input_data * weights['node_1']).sum()  
> node_1_output = np.tanh(node_1_input)  
> hidden_layer_outputs = np.array([node_0_output, node_1_output])  
> output = (hidden_layer_output * weights['output']).sum()  
> print(output)  
> [out1.238]

# Deeper networks

## Representation learning
- deep networks internally build representations of patterns in the data
- partially replace the need for future engineering
- subsequent layers build increasingly sophisticated representations of raw data
