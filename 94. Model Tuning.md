# Tuning a CART's hyperparameters
## Hyperparameters
Machine learning model:
- parameters: learned from data
  - CART example: split-point of a node, split-feature of a node, ...
- hyperparameters: not learned from data, set prior to training
  - CART example: max_depth, min_samples_leas, splitting criterion

## What is hyperparameter tuning?
- problem: search for a set of optimal hyperparameters for a learning algorithm
- solution: find a set of optimal hyperparameters that results in an optimal model
- optimal models: yield an optimal score
- score: in sklearn defaults to accuracy(classification) and $R^2$ (regression)
- cross validation is used to estimate the generalization performance

## Why tune hyperparameters?
- in sklearn, a model's defalut hyperparameters are not optimal for all problems
- hyperparameters should be tuned to obtain the best model performance

## Approaches to hyperparameter tuning
- grid search
- random search
- Bayesian Optimization
- Genetic algorithms
- ...

## Grid search cross validation
- manually set a grid of discrete hyperparameter values
- set a metric for scoring model performance
- search exhaustively through the grid
- for each set of hyperparameters, evaluate each model's CV score
- the optimal hyperparameters are those of the model achieving the best CV score

## Grid search cross validation: example
- hyperparameters grids:
  - max_depth = {2,3,4}
  - min_samples_leaf = {0.05, 0.1}
- hyperparameter space = {(2, 0.05), (2, 0.1), (3, 0.05), ...}
- CV score = {$score_{(2, 0.05)}$, ...}
- optimal hyperparameters = set of hyperparameters corresponding to the best CV score.

**inspecting the pyperparameters of a CART in sklearn, get_params()_**
> #import DecisionTreeClassifier  
> from sklearn.tree import DecisionTreeClassifier  
> seed = 1  
> #instantiate a decisionTreeClassifier 'dt'  
> dt = DecisionTreeClassifier(random_state = seed)  
> #print out 'dt's hyperparameters  
> print(dt.get_params())  
> [out]...

__Grid search CV in sklearn(Breast Cancer dataset)__
> #import GridSearchCV  
> from sklearn.model_selection import GridSearchCV  
> #define the grid of gyperparameters 'params_dt'  
>  params_dt = {'max_depth' : [3,4,5,6], 'min_samples_leaf' : [0.04, 0.06, 0.08], 'max_features': [0.2, 0.4, 0.6, 0.8]}  
> #instantiate a 10-fold CV grid search object 'grid_dt'  
> grid_dt = GridSearchCV(estimator = dt, __param_grid = params_dt__, scoring = 'accuracy', cv = 10, n_jobs = -1)  
> #fit 'grid_dt' to the training data  
> grid_dt.fit(X_train, y_train)

**Extracting the best hyperparameters(best_params_, best_score_)**
> #extract best hyperparameters from 'grid_dt'  
> best_hyperparams = grid_dt.__best_params___  
> print('best hyperparameters:\n', best_hyperparams)  
> [out] best hyperparameters:  
> {'max depth' : 3, 'max_features': 0.4, 'min_samples_leaf': 0.06}  
> #extract best CV score from 'grid_dt'  
> best_CV_score = grid_dt.__best_score__  
> print('best CV accuracy'.format(best_CV_score))  
> [out] best CV accuracy: 0.938

**Extracting the best estimator, best_estimator_**
> #extract best model from 'grid_dt'  
> best _model = grid_dt.best_estimator_  
> #evaluate test set accuracy  
> test_acc = best_model.score(X_test, y_test)  
> print('test set accuracy of best model: {:.3f}.format(test_acc))  
> [out] test set accuracy of best model: 0.947

# Tuning an RF's Hyperparameters
## Random Forests Hyperparameters
- CART hyperparameters
- number of estimators
- bootstrap

## Tuning is expensive
- Hyperparameter tuning:
  - computationally expensive
  - sometimes leads to very slight improvement
- Weight the impact of tuning on the whole project

**inspecting RF hyperparameters in sklearn, get_params()**
> from sklearn.ensemble import RandomForestRegressor  
> seed = 1  
> rf = RandomForestRegressor(random_state = seed  
> #inspect rf's hyperparameters  
> rf.get_params()  
> [out] ...

__GridSearchCV in sklearn (auto dataset)__
> from sklearn.metrics import mean_squared_error as MSE  
> from sklearn.model_selection import GridSearchCV  
> #define a grid of hyperparameter 'params_rf'  
> params_rf = {'n_estimators':[300,400,500], 'maz_depth':[4,6,8], 'min_samples': [0.1, 0.2], 'max_features':['log2, 'sqrt']}  
> #instantiate 'grid_rf'  
> grid_rf = GridSearchCV(estimator = rf, param_grid = params_rf, scoring = 'neg_mean_squared_error', verbose = 1, n_jobs = -1)  

__searching for the best hyperparameters__
> #fit 'gird_rf' to the training set  
> grid_rf.fit(X_train, y_train)  
> [out] ...

**Extracting the best hyperparameters, best_params_**
> best_hyperparams = grid_rf.best_params_  
> print(best hyperparameters, best_hyperparams)  
> [out]...

**Evaluating the best model performance, best_estimator_**  
> best_model = grid_rf.best_estimator_  
> y_pred = best_model.predict(X_test)  
> rmse_test = MSE(y_test, y_pred)**(1/2)  
> print('test set RMSE of rf: {:.2f}'.format(rmse_test))  
> test set RMSE of rt: 3.83