# Bagging
## Ensemble Methods
__Voting Classifier__
- same training set
- $\neq$ algorithms

__Bagging__
- one algorithm
- $\neq$ subsets of the training set

## Bagging
- bagging: bootstrap aggregation
- uses a technique known as the bootstrap
- reduces variance of individual models in the ensemble

## Bagging: Classification & Regression
__Classification__
- aggregates predictions by majority voting
- _BaggingClassifier_ in scikit-learn

__Regression__
- aggregate predictions through averaging
- _BaggingRegressor_ in scikit-learn

__Bagging Classifier in sklearn(Breast-Cancer Dataset)__

> #import models and utility functions  
> from sklearn.ensemble import BaggingClassifier  
> from sklearn.tree import DecisionTreeClassifier  
> from sklearn.metrics import accuracy_score  
> from sklearn.model_selection import train_test_split  
> seed = 1  
> X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, stratify = y, random_state = seed)

> #instantiate a classification-tree 'dt'  
> dt = DecisionTreeClassifier(max_depth = 4, min_samples_leaf = 0.16, random_state = seed)  
> #instantiate a BaggingClassifier 'bc'  
> bc = BaggingClassifier(base_estimator = dt, n_estimators = 300, n_jobs = -1)  
> bc.fit(X_train, y_train)  
> y_pred = bc.predict(X_test)  
> accuracy = accuracy_score(y_test, y_pred)  
> print('accuracy of bagging classifier: {:.3f}'.format(accuracy))  
> [out] accuracy of bagging classifier: 0.936

# Out of Bag Evaluation
## Bagging
- some instances my be sampled several times for one model,
- other instances may not be sampled at all

## Out Of Bag(OOB) instances
- on average, for each model, 63% of training instances are sampled.
- the remaining 37% constitute the OOB instances

__OOB evaluation in sklearn__
> from sklearn.ensemble import BaggingClassifier  
> from sklearn.tree import DecisionTreeClassifier  
> from sklearn.metrics import accuracy_score  
> from sklearn.model_selection import train_test_split  
> seed = 1  
> X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, stratify = y, random_state = seed)

> #instantiate a classification-tree 'dt'  
> dt = DecisiontreeClassifier(max_depth = 4,min_samples_leaf = 0.16, random_state = seed)  
> #instantiate a BaggingClassifier 'bc', set oobscore = Ture  
> bc = BaggingClassifier(base_estimator = dt, n_estimators = 300, oob_score = True, n_jobs = -1)  
> bc.fit(X_train, y_train)  
> y_pred = bc.predict(X_test)  

> test_accuracy = accuracy_score(y_test, y_pred)  
> #extract the OOB accuracy from 'bc'  
> oob_accuracy = bc.oob_score_  
> print('test set accuracy: {:.3f}'.format(test_accuracy))  
> [out] test set accuracy: 0.936  
> print('OOB accuracy: {:.3f}'.format(oob_accuracy))  
> [out] OOB accuracy: 0.925

# Random Forest
## Bagging
- base estimator: Decision Tree, Logistic Regression, Neural Net, ...
- each estimator is trained on a distinct bootstrap sample of the training set
- estimators use all features for training and prediction

## Further Diversity with Random Forests
- base estimator: Decision Tree
- each estimator is trained on a different bootstrap sample having the same size as the training set
- RF introduces further randomization in the training of individual trees
- $d$ features are sampled at each node without replacement ($d < total$ $number$ $of$ $featues$)

## Random Forests: Classification & Regression
__Classification__
- aggregates predictions by majority voting
- _RandomForestClassifier_ in scikit-learn

__Regression__
- aggregrates predictions thourgh averaging
- _RandomForestRegressor_ in scikit-learn

__Random Forest Regressor in sklaern(auto dataset)__

> from sklearn.ensemble import RandomForestRegressor  
> from sklearn.model_selection import train_test_split  
> from sklearn.metrics import mean_squared_error as MSE  
> seed = 1  
> X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = seed)

> #instantiate a random forest regressor 'rf' 400 estimators  
> rf = RandomForestRegressor(n_estimators = 400, min_samples_leaf = 0.12,random_state = seed)  
> #fit 'rf' to the training set  
> rf.fit(X_train, y_train)  
> y_pred = rf.predict(X_test)  
> #evaluate the test set RMSE  
> rmse_test = MSE(y_test, y_pred)**(0.5)  
> print('test set RMSE of rf: {:.2f}'.format(rmse_test))  
> [out] test set RMSE of rf: 3.98

## Feature importance
Tree-based method: enable measuring the importance of each feature in prediction.  
in sklearn:
- how much the tree nodes use a particular feature(weighted average) to reduce impurity
- accessed using the attribute _feature_importance__

__Feature importance in sklearn__  
> import pandas as pd  
> import matplotlib.pyplot as plt  
> #create a pd.Series of features importances  
> importances_rf = pd.Series(rf.feature_importances_, index = X.columns)  
> #sort importances_rf_  
> sorted_importances_rf = importance_rf.sort_value()  
> #make a horizontal bar plot  
> sorted_importances_rf.plot(kind = 'barh', color = 'lightgreen')  
> plt.show()