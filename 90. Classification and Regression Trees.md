# Decision-Tree for Classification

## Course Overview
- Chapter 1: Classification and Regression Tree (CART)
- Chapter 2: The Bias-Variance Tradeoff
- Chapter 3: Bagging and Random Forests
- Chapter 4: Boosting
- Chapter 5: Model Tuning

## Classification-Tree
- sequence of if-else questions about individual features
- Objective: infer class labels
- able to capture non-linear relationships between features and labels
- don't require feature scaling(ex: Standardization...)

__Classification-tree in scikit-learn__
> from sklearn.tree import DecisionTreeClassifier  
> from sklearn.model_selection import train_test_split  
> from sklearn.metrics import accuracy_score  
> X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y, random_state = 1)  
> #instantiate dt  
> df = DecisionTreeclassifier(max_depth = 2, random_state = 1)

__Classification-tree in scikit-learn__
>#fit dt to the training set  
> dt.fit(X_train, y_train)  
>#predict test set labels  
> y_pred = dt.predict(X_test)  
>#evaluate test-set accuracy  
> accuracy_score(y_test, y_pred)  
> [out] 0.90

## Decision Regions
__Decision Region__: region in the feature space where all instances are assigned to one class label.

__Decision Boundary__: surface separating different decision regions

# Classification-Tree Learning
## Building Blocks of a Decision-Tree
- Decision-Tree: data structure consisting of a hierarchy of nodes
- Node: question or prediction

<br>

__Three kinds of nodes__
- root: no parent nodem question giving rise to two childern nodes.
- internal node: one parent node, question giving rise to two children nodes.
- leaf: one parent node. no children nodes = prediction

## Information Gain(IG)
criteria to measure the impunity of a node I(node)
- gain index
- entropy...
## Classification-Tree Learning
- nodes are grown recursively
- at each node, split the data based on:
  - feature f and split-point sp to maximaze IG(node)
  - if IG(node) = 0, declare the node a leaf

__information criterion in scikit-learn (breast cancer dataset)__
> from sklearn.tree import DecisionTreeClassifier  
> from sklearn.model_selection import train_test_split  
> from sklearn.metrics import accuracy_score  
> X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y, random_state = 1)  
> dt = DecisionTreeClassifier(criterion = 'gini', random_state = 1) 

> dt.fit(X_train, y_train)  
> y_pred = dt.predict(X_test)  
> accuracy_score(y_test, y_pred)  
> [out] 0.921

# Decision-Tree for Regression
__Regression-Tree in scikit-learn__
> from sklearn.tree import DecisionTreeRegressor  
> from sklearn.model_selection import train_test_split  
> from sklearn.metrics import mean_squared_error as MSE  
> X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 3)  
> #instantiate a DecisionTreeRegressor 'dt'  
> dt = DecisionTreeRegressor(max_depth = 4, min_samples_leaf = 0.1, random_state = 3)

> dt.fit(X_train, y_train)  
> y_pred = dt.predict(X_test)  
> #compute test-set MSE  
> mse_dt = MSE(y_test, y_pred)  
> #compute test-set RMSE  
> rmse_dt = mse_dt**(1/2)  
> print(rmse_dt)